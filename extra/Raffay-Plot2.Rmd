---
title: "Raffay Plot 2"
output: html_document
---

### Introduction
Our second question aims to explore the difference, if any, between ads aired during election years compared to those aired during non-election years. Particularly, we want to analyze if there is any noticeable difference in the description and content between the election and non-election year ads. Answering this question involves looking at the `title` and `description` variables to analyze how these ads were described, and then analyzing the boolean content variables of `use_sex`, `patriotic`,`funny`,`celebrity`,`danger`, and `animals` to see if they had any noticeable differences in their content. Finally, the `year` variable is also needed to distinguish between election and non-election years. 

We are interested in exploring this question because election years mark a significant cultural moment in the US. Therefore, we want to see whether this focus towards politics transfers to super bowl ads too. 


### Approach 

To analyze the description aspect of our question, we decided to use a word cloud visualization since we felt it was the most informative way to visualize what major descriptors are used for the ads. The alternative way of analyzing `title` and `description` that we considered included a bar char for top 10-20 words. However, we decided to opt for the word cloud since it provided more information (i.e number of words) and better captured the relative position of all the words being used (by size).

```{r}
library(tidyverse)
library(wordcloud)
library(RColorBrewer)
library(tm)
library(wordcloud2)

knitr::opts_chunk$set(echo = TRUE)
youtube <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-03-02/youtube.csv', show_col_types = FALSE)

election_year <- c(2000,2004,2008,2012,2016,2020)
youtube <- youtube %>% 
  mutate(election_years = ifelse(year %in% election_year,1,0)) 

test <- youtube %>% 
          filter(election_years == 1) %>% 
          select(title)

docs <- Corpus(VectorSource(test))
docs
docs <- docs %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removeWords, c("super","bowl","commercial","superbowl",
                                    "bud","light","budweiser","pepsi",
                                    "hyundai","doritos","coke","cocacola",
                                    "cola","coca","kia","toyota"))
dtm <- TermDocumentMatrix(docs) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words)

set.seed(1234) # for reproducibility 
wordcloud(words = df$word, 
          freq = df$freq,
          min.freq = 1,
          max.words=100, 
          random.order=FALSE, 
          rot.per=0.35,
          colors=brewer.pal(8, "Dark2"))

# Next Steps
# Filter for Brand Names 
# Make Frequency Charts for the attributes 


# Way to create summary table with column names 
election_yr <- youtube %>% 
  pivot_longer(cols = c(use_sex,funny,celebrity,patriotic,danger,animals)) %>% 
  filter(election_years == 1) %>% 
  filter(value == TRUE) %>% 
  group_by(name) %>% 
  summarise(n = n()) %>% 
  mutate(perc = paste(round((n / sum(n))*100),"%",sep = ""))

election_yr %>% 
  ggplot(aes(y = name, x = n, fill = name)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = perc, color = name), nudge_x = 2, show.legend = FALSE) +
  labs(
    x = "Count",
    y = "Ad Attribute",
    title = "Count and Proportion of Ad by Attribute",
    subtitle = "In Election Years"
  ) +
  theme_minimal() +
  scale_fill_manual(values = c("Grey","Grey","Grey","Grey","Red","Grey")) +
  scale_color_manual(values = c("Grey","Grey","Grey","Grey","Red","Grey"))
 


```

```{r}


test2 <- youtube %>% 
          filter(election_years == 0) %>% 
          select(title)

docs <- Corpus(VectorSource(test2))
docs
docs <- docs %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, c("super","bowl","commercial","superbowl",
                                    "bud","light","budweiser","pepsi",
                                    "hyundai","doritos","coke","cocacola",
                                    "cola","coca","the","kia"))
dtm <- TermDocumentMatrix(docs) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words)

set.seed(1234) # for reproducibility 
wordcloud(words = df$word, 
          freq = df$freq,
          min.freq = 1,
          max.words=100, 
          random.order=FALSE, 
          rot.per=0.35,
          colors=brewer.pal(8, "Dark2"))

wordcloud2(data=df, size=1.6, color='random-dark')

# Next Steps
# Filter for Brand Names 
# Make Frequency Charts for the attributes 


```



```{r}

# Way to create summary table with column names 
# Way to create summary table with column names 
no_election_yr <- youtube %>% 
  pivot_longer(cols = c(use_sex,funny,celebrity,patriotic,danger,animals)) %>% 
  filter(election_years == 0) %>% 
  filter(value == TRUE) %>% 
  group_by(name) %>% 
  summarise(n = n()) %>% 
  mutate(perc = paste(round((n / sum(n))*100),"%",sep = "")) 

no_election_yr %>% 
  ggplot(aes(y = name, x = n, fill = name)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = perc, color = name), nudge_x = 3.5, show.legend = FALSE) +
  labs(
    x = "Count",
    y = "Ad Attribute",
    title = "Count and Proportion of Ad by Attribute",
    subtitle = "In Election Years"
  ) +
  theme_minimal() +
  scale_fill_manual(values = c("Grey","Grey","Grey","Grey","Red","Grey")) +
  scale_color_manual(values = c("Grey","Grey","Grey","Grey","Red","Grey"))
 

```


```{r ggplot-wordcloud}

library(ggwordcloud)

df %>% 
  filter(freq >= 2) %>% 
  ggplot(aes(label = word, 
             color = factor(sample.int(10, 76, 
                                       replace = TRUE)),
             size = freq)) +
  geom_text_wordcloud()+
  scale_size_area(max_size = 10)
  theme_minimal()
      

  


```

https://cran.r-project.org/web/packages/ggwordcloud/vignettes/ggwordcloud.html

